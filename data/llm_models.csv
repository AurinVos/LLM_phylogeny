name,family,release_month,influences,innovation
Attention Is All You Need,Root,1-6-2017,,Introduced the Transformer architecture with multi-head self-attention and positional encoding
GPT-1,GPT,1-6-2018,Attention Is All You Need,Applied decoder-only Transformer with generative pretraining on BooksCorpus
BERT,Encoder-only,1-10-2018,Attention Is All You Need,Bidirectional encoder pretraining with masked language modelling and next sentence prediction
GPT-2,GPT,1-2-2019,GPT-1,Scaled decoder-only models with zero-shot transfer via WebText
Transformer-XL,Long-context,1-1-2019,Attention Is All You Need,Segment-level recurrence and relative positional encoding for long-context modelling
XLNet,Encoder-only,1-6-2019,Transformer-XL; BERT,Permuted language modelling objective blending autoregressive and autoencoding pretraining
Megatron-LM,Scaling,1-8-2019,GPT-2,Model parallelism for trillion-parameter scale using tensor and pipeline parallelism
RoBERTa,Encoder-only,1-7-2019,BERT,"Optimised masked language modelling with longer training, dynamic masking, and larger batches"
ALBERT,Encoder-only,1-9-2019,BERT,Parameter sharing and factorised embeddings for lightweight bidirectional transformers
BART,Seq2Seq,1-10-2019,Attention Is All You Need,Denoising autoencoder that bridges encoder-decoder pretraining for text generation
T5,Seq2Seq,1-10-2019,Attention Is All You Need,Text-to-Text framework with unified transfer learning and span-corruption objective
ELECTRA,Encoder-only,1-3-2020,BERT,Replaced masked language modelling with discriminator that detects replaced tokens
GPT-3,GPT,1-5-2020,GPT-2,175B parameter scaling with in-context learning across diverse tasks
Switch Transformer,Mixture-of-Experts,1-1-2021,T5,Sparse mixture-of-experts routing enabling trillion-parameter efficiency
LaMDA,Google,1-5-2021,Attention Is All You Need,Dialogue-optimised training with safety fine-tuning and grounded responses
Gopher,DeepMind/Scaling,1-12-2021,Attention Is All You Need,Scaling laws with retrieval-style evaluation and precision study for large transformers
GLaM,Mixture-of-Experts,1-12-2021,Switch Transformer,Hierarchical mixture-of-experts with token-level routing across 1.2T parameters
Chinchilla,DeepMind/Scaling,1-3-2022,Gopher,Data/parameter scaling law balancing showing benefits of more tokens over parameters
PaLM,Google,1-4-2022,LaMDA,Pathways system with parallelism and chain-of-thought prompting across 540B parameters
UL2,Seq2Seq,1-5-2022,T5,Mixture-of-denoisers objective supporting multiple corruption schemes for unified learning
OPT,Open-Source GPT,1-5-2022,GPT-3,Reproducible GPT-3 class model with fully documented training pipeline
BLOOM,Open-Source GPT,1-7-2022,GPT-3; Megatron-LM,Multilingual open-access 176B parameter model trained collaboratively via Megatron-DeepSpeed
LLaMA,LLaMA,1-2-2023,Chinchilla,"Efficient scaling via smaller datasets, grouped-query attention, and open research weights"
GPT-4,GPT,1-3-2023,Chinchilla; GPT-3,Large multimodal alignment with reinforced fine-tuning and tool integration
InstructGPT,Alignment,1-1-2022,GPT-3,Reinforcement learning from human feedback tailored to instruction following
Constitutional AI,Alignment,1-12-2022,InstructGPT,Self-critiquing alignment loop guided by explicit normative principles
Claude 1,Claude,1-3-2023,Constitutional AI,Applied constitutional AI feedback to align helpful and harmless behaviour
Baichuan-7B,Baichuan,1-6-2023,LLaMA,Chinese-English bilingual adaptation of LLaMA with extended vocabulary
Claude 2,Claude,1-7-2023,Claude 1,Expanded context and constitutional alignment refinements with tool use
Llama 2,LLaMA,1-7-2023,LLaMA,Open-weight release with supervised fine-tuning and RLHF safety tuning
InternLM-7B,InternLM,1-7-2023,LLaMA,Toolkit-oriented Chinese open model with multi-stage pretraining and alignment
Claude Instant 1.2,Claude,1-8-2023,Claude 1,Latency-optimised Claude variant retaining constitutional safety guarantees
Qwen 7B,Qwen,1-8-2023,Chinchilla; LLaMA,Chinese-English foundation with rotary embeddings and fine-grained tokeniser
Mistral 7B,Mistral,1-9-2023,Chinchilla; LLaMA,Sliding window attention and grouped-query attention for efficient small models
Baichuan2,Baichuan,1-9-2023,Baichuan-7B,Improved bilingual data curation with extended context and tool APIs
Claude 2.1,Claude,1-11-2023,Claude 2,Higher factual reliability and longer context for enterprise tasks
Yi-34B,Yi,1-11-2023,Llama 2,Balanced bilingual dataset with progressive context extension
Mixtral 8x7B,Mistral,1-12-2023,Mistral 7B,Sparse mixture-of-experts combining eight Mistral experts with router training
InternLM2-7B,InternLM,1-1-2024,InternLM-7B,Iterative distillation with modular tool-using skills and expanded context
Baichuan3,Baichuan,1-1-2024,Baichuan2,Domain-adaptive pretraining with knowledge-augmented decoding
Gemini 1.5,Google,1-2-2024,PaLM,Mixture-of-experts multimodal model with million-token context
Claude 3 Haiku,Claude,1-3-2024,Claude 2.1,Fast multimodal assistant with revised constitutional tuning
Claude 3 Sonnet,Claude,1-3-2024,Claude 2.1,Mid-tier multimodal reasoning with tool orchestration
Claude 3 Opus,Claude,1-3-2024,Claude 2.1,Flagship Claude with state-of-the-art reasoning and coding alignment
Llama 3,LLaMA,1-4-2024,Llama 2,Token-efficient vocabulary and speculatively decoded training mix
GPT-4o,GPT,1-5-2024,GPT-4,Unified multimodal end-to-end model with real-time streaming latency
Claude 3.5 Sonnet,Claude,1-6-2024,Claude 3 Sonnet,Improved tool-use reliability and creative reasoning
Qwen2,Qwen,1-6-2024,Qwen 7B,Data mixture refresh with extended context and reasoning tuning
Llama 3.1,LLaMA,1-7-2024,Llama 3,Multi-token prediction and improved tool-calling APIs
DeepSeek-V2.5,DeepSeek,1-9-2024,LLaMA,Sparse mixture-of-experts with hybrid reinforcement learning
Claude 3.5 Haiku,Claude,1-10-2024,Claude 3 Haiku,Faster multimodal responses with improved grounding
DeepSeek-V3,DeepSeek,1-12-2024,DeepSeek-V2.5,Unified MoE and dense experts with reinforcement fine-tuning
Qwen2.5 (Max),Qwen,1-1-2025,Qwen2,Expanded multilingual coverage with large context generalisation
DeepSeek-R1,DeepSeek,1-1-2025,DeepSeek-V3,Reinforced reasoning with reward models targeting mathematical proofs
Claude 3.7 Sonnet,Claude,1-2-2025,Claude 3.5 Sonnet,Long-context orchestration and improved agentic behaviours
Qwen3,Qwen,1-4-2025,Qwen2.5 (Max),Mixture-of-experts scaling with automated reasoning curriculum
Claude 4 Sonnet,Claude,1-5-2025,Claude 3.7 Sonnet,Structured tool-use planning with autonomous memory
Claude 4 Opus,Claude,1-5-2025,Claude 3 Opus,Frontier reasoning with multi-agent constitutional guidance
GPT-5,GPT,1-8-2025,GPT-4o,Next-generation multimodal orchestration with autonomous tool chains
Claude Opus 4.1,Claude,1-8-2025,Claude 4 Opus,Iterative self-improvement through agentic evaluation loops
Claude 4.5 Sonnet,Claude,1-9-2025,Claude 4 Sonnet,Hybrid symbolic-neural planning with compressed memory
